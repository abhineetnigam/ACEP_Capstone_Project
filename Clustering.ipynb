{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRWO7kLMwXkD"
   },
   "source": [
    "69kV substation\n",
    "\n",
    "Residentail\t8699.960125\n",
    "\n",
    "Commercial\t1513.142271\n",
    "\n",
    "Industrial\t80.33114282\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWPD0gYhYN2S"
   },
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ulqW0FYBgjye",
    "outputId": "6b7f0312-38e1-4ce9-b3d4-9f56c5017966"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "#https://github.com/ZwEin27/Hierarchical-Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "22DeEIPLh9PV"
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "import os\n",
    "import heapq\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_-FV5BkEiPrs",
    "outputId": "3b89f5a2-b81a-4449-f6b1-3e3960f54d3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ind_cent 1025\n",
      "ind_gvea 389\n",
      "com_cent 21991\n",
      "com_gvea 5775\n",
      "res_cent 60632\n",
      "res_gvea 3965\n"
     ]
    }
   ],
   "source": [
    "#import industrial data for KNN clustering\n",
    "ind_cent = pd.read_csv('/content/drive/MyDrive/ACEP_Data_Team/Railbelt_line/Script_data/CSV_files/Industrial_divided/Central.csv', low_memory=False)\n",
    "ind_gvea=pd.read_csv('/content/drive/MyDrive/ACEP_Data_Team/Railbelt_line/Script_data/CSV_files/Industrial_divided/GVEA.csv', low_memory=False)\n",
    "\n",
    "com_cent=pd.read_csv('/content/drive/MyDrive/ACEP_Data_Team/Railbelt_line/Script_data/CSV_files/Commercial_divided/com_central.csv', low_memory=False)\n",
    "com_gvea=pd.read_csv('/content/drive/MyDrive/ACEP_Data_Team/Railbelt_line/Script_data/CSV_files/Commercial_divided/com_GVEA.csv', low_memory=False)\n",
    "\n",
    "res_cent =pd.read_csv('/content/drive/MyDrive/ACEP_Data_Team/Railbelt_line/Script_data/CSV_files/Res_Divided/res_central.csv', low_memory=False)\n",
    "res_gvea =pd.read_csv('/content/drive/MyDrive/ACEP_Data_Team/Railbelt_line/Script_data/CSV_files/Res_Divided/res_gvea.csv', low_memory=False)\n",
    "\n",
    "df_list=[ind_cent,ind_gvea,com_cent,com_gvea,res_cent,res_gvea]\n",
    "df_string=[\"ind_cent\",\"ind_gvea\",\"com_cent\",\"com_gvea\",\"res_cent\",\"res_gvea\"]\n",
    "for ind in range(len(df_list)):\n",
    "  df=df_list[ind]\n",
    "  print(df_string[ind],len(df_list[ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBE5gjtxlLuG"
   },
   "outputs": [],
   "source": [
    "# frames = [ind_cent,ind_gvea, com_cent,com_gvea, res_cent,res_gvea]\n",
    "\n",
    "# combined = pd.concat(frames)\n",
    "# len(combined)\n",
    "# combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_TzyqCNL-Ir"
   },
   "source": [
    "\n",
    "# Hierarchical Clustering From Scratch\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_Gq1j9uTEjE"
   },
   "source": [
    "## Hierarchical Clustering Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WeeVsJTjSl-4"
   },
   "outputs": [],
   "source": [
    "class Hierarchical_Clustering:\n",
    "    def __init__(self, ipt_data, max_size):\n",
    "        self.input_file_name = ipt_data\n",
    "        self.dataset = None\n",
    "        self.dataset_size = 0\n",
    "        self.dimension = 0\n",
    "        self.cluster_max_size = max_size\n",
    "        self.heap = []\n",
    "        self.clusters = []\n",
    "    \n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"\n",
    "        Initialize and check parameters\n",
    "        \"\"\"\n",
    "        # check file exist and if it's a file or dir\n",
    "        # if not os.path.isfile(self.input_file_name):\n",
    "        #     self.quit(\"Input file doesn't exist or it's not a file\")\n",
    "\n",
    "        self.dataset, self.clusters = self.load_data(self.input_file_name)\n",
    "        self.dataset_size = len(self.dataset)\n",
    "\n",
    "        if self.dataset_size == 0:\n",
    "            self.quit(\"Input file doesn't include any data\")\n",
    "\n",
    "        self.dimension = len(self.dataset[0][\"data\"])\n",
    "\n",
    "        if self.dimension == 0:\n",
    "            self.quit(\"dimension for dataset cannot be zero\")\n",
    "\n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    \"\"\"                      Hierarchical Clustering Functions                       \"\"\"\n",
    "    \"\"\"                                                                              \"\"\"    \n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "    def euclidean_distance(self, data_point_one, data_point_two):\n",
    "        \"\"\"\n",
    "        euclidean distance: https://en.wikipedia.org/wiki/Euclidean_distance\n",
    "        assume that two data points have same dimension\n",
    "        \"\"\"\n",
    "        size = len(data_point_one)\n",
    "        result = 0.0\n",
    "        for i in range(size):\n",
    "            f1 = float(data_point_one[i])   # feature for data one\n",
    "            f2 = float(data_point_two[i])   # feature for data two\n",
    "            tmp = f1 - f2\n",
    "            result += pow(tmp, 2)\n",
    "        result = math.sqrt(result)\n",
    "        return result\n",
    "\n",
    "    def compute_pairwise_distance(self, dataset):\n",
    "        result = []\n",
    "        dataset_size = len(dataset)\n",
    "        for i in range(dataset_size-1):    # ignore last i\n",
    "            for j in range(i+1, dataset_size):     # ignore duplication\n",
    "                dist = self.euclidean_distance(dataset[i][\"data\"], dataset[j][\"data\"])\n",
    "                result.append( (dist, [dist, [[i], [j]]]) )\n",
    "\n",
    "        return result\n",
    "                \n",
    "    def build_priority_queue(self, distance_list):\n",
    "        heapq.heapify(distance_list)\n",
    "        self.heap = distance_list\n",
    "        return self.heap\n",
    "\n",
    "    def compute_centroid_two_clusters(self, current_clusters, data_points_index):\n",
    "        size = len(data_points_index)\n",
    "        dim = self.dimension\n",
    "        centroid = [0.0]*dim\n",
    "        for index in data_points_index:\n",
    "            dim_data = current_clusters[str(index)][\"centroid\"]\n",
    "            for i in range(dim):\n",
    "                centroid[i] += float(dim_data[i])\n",
    "        for i in range(dim):\n",
    "            centroid[i] /= size\n",
    "        return centroid\n",
    "\n",
    "    def compute_centroid(self, dataset, data_points_index):\n",
    "        size = len(data_points_index)\n",
    "        dim = self.dimension\n",
    "        centroid = [0.0]*dim\n",
    "        for idx in data_points_index:\n",
    "            dim_data = dataset[idx][\"data\"]\n",
    "            for i in range(dim):\n",
    "                centroid[i] += float(dim_data[i])\n",
    "        for i in range(dim):\n",
    "            centroid[i] /= size\n",
    "        return centroid\n",
    "\n",
    "    def hierarchical_clustering(self):\n",
    "        \"\"\"\n",
    "        Main Process for hierarchical clustering\n",
    "        \"\"\"\n",
    "        dataset = self.dataset\n",
    "        current_clusters = self.clusters\n",
    "        old_clusters = []\n",
    "        heap = self.compute_pairwise_distance(dataset)\n",
    "        heap = self.build_priority_queue(heap)\n",
    "\n",
    "        while True:\n",
    "            dist, min_item = heapq.heappop(heap)\n",
    "            if dist >= float('inf'):\n",
    "                break\n",
    "            # pair_dist = min_item[0]\n",
    "            pair_data = min_item[1]\n",
    "            # judge if include old cluster\n",
    "            if not self.valid_heap_node(min_item, old_clusters):\n",
    "                continue\n",
    "\n",
    "            new_cluster = {}\n",
    "            new_cluster_elements = sum(pair_data, [])\n",
    "\n",
    "            if len(new_cluster_elements) > self.cluster_max_size:\n",
    "                dist = float('inf')\n",
    "                heapq.heappush(heap, (dist, [dist, pair_data]))\n",
    "                continue\n",
    "            new_cluster_cendroid = self.compute_centroid(dataset, new_cluster_elements)\n",
    "            new_cluster_elements.sort()\n",
    "            new_cluster.setdefault(\"centroid\", new_cluster_cendroid)\n",
    "            new_cluster.setdefault(\"elements\", new_cluster_elements)\n",
    "            for pair_item in pair_data:\n",
    "                old_clusters.append(pair_item)\n",
    "                del current_clusters[str(pair_item)]\n",
    "            self.add_heap_entry(heap, new_cluster, current_clusters)\n",
    "            current_clusters[str(new_cluster_elements)] = new_cluster\n",
    "        return current_clusters\n",
    "            \n",
    "    def valid_heap_node(self, heap_node, old_clusters):\n",
    "        pair_dist = heap_node[0]\n",
    "        pair_data = heap_node[1]\n",
    "        for old_cluster in old_clusters:\n",
    "            if old_cluster in pair_data:\n",
    "                return False\n",
    "        return True\n",
    "            \n",
    "    def add_heap_entry(self, heap, new_cluster, current_clusters):\n",
    "        for ex_cluster in current_clusters.values():\n",
    "            new_heap_entry = []\n",
    "            dist = self.euclidean_distance(ex_cluster[\"centroid\"], new_cluster[\"centroid\"])\n",
    "            new_heap_entry.append(dist)\n",
    "            new_heap_entry.append([new_cluster[\"elements\"], ex_cluster[\"elements\"]])\n",
    "            heapq.heappush(heap, (dist, new_heap_entry))\n",
    "\n",
    "\n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    \"\"\"                             Helper Functions                                 \"\"\"\n",
    "    \"\"\"                                                                              \"\"\"    \n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    def load_data(self, input_file):\n",
    "        \"\"\"\n",
    "        load data and do some preparations\n",
    "        \"\"\"\n",
    "        #input_file = open(input_file_name, 'rU')\n",
    "        dataset = []\n",
    "        clusters = {}\n",
    "        id = 0\n",
    "        for row in input_file:\n",
    "            # line = line.strip('\\n')\n",
    "            # row = str(line)\n",
    "            # row = row.split(\",\")\n",
    "            iris_class =0\n",
    "\n",
    "            data = {}\n",
    "            data.setdefault(\"id\", id)   # duplicate\n",
    "            data.setdefault(\"data\", row)\n",
    "            data.setdefault(\"class\", iris_class)\n",
    "            dataset.append(data)\n",
    "\n",
    "            clusters_key = str([id])\n",
    "            clusters.setdefault(clusters_key, {})\n",
    "            clusters[clusters_key].setdefault(\"centroid\", row)\n",
    "            clusters[clusters_key].setdefault(\"elements\", [id])\n",
    "\n",
    "\n",
    "\n",
    "            id += 1\n",
    "        return dataset, clusters\n",
    "\n",
    "    def quit(self, err_desc):\n",
    "        raise SystemExit('\\n'+ \"PROGRAM EXIT: \" + err_desc + ', please check your input' + '\\n')\n",
    "\n",
    "    def loaded_dataset(self):\n",
    "        \"\"\"\n",
    "        use for test only\n",
    "        \"\"\"\n",
    "        return self.dataset\n",
    "\n",
    "    def display(self, current_clusters, dataset):\n",
    "        colors = plt.cm.rainbow(np.linspace(0, 1, len(current_clusters)))\n",
    "        plt.figure(figsize=(15,15))\n",
    "        for ind, (_, clusterDict) in enumerate(current_clusters.items()):\n",
    "            elems = clusterDict['elements']\n",
    "            centroid = clusterDict['centroid']\n",
    "            plt.scatter(dataset[elems][:, 0], dataset[elems][:, 1], color=colors[ind], marker='x', s=10)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XEQtRnPGqjkU"
   },
   "outputs": [],
   "source": [
    "#Plotting the Clusters\n",
    "def display(current_clusters, dataset):\n",
    "        colors = plt.cm.rainbow(np.linspace(0, 1, len(current_clusters)))\n",
    "        plt.figure(figsize=(10,15))\n",
    "        for ind, (_, clusterDict) in enumerate(current_clusters.items()):\n",
    "            elems = clusterDict['elements']\n",
    "            centroid = clusterDict['centroid']\n",
    "            plt.scatter(dataset[elems][:, 0], dataset[elems][:, 1], color=colors[ind], marker='x', s=10, label=ind)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15A8uD_JGdXp"
   },
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\"\"\"                               Main Method                                    \"\"\"    \n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "def MainMethod(X_gvea,size):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    - ipt_data: a text file name for the input data\n",
    "    - imax_size  Max size of cluster\n",
    "    \"\"\"\n",
    "    ipt_data = X_gvea      # input data, e.g. iris.dat\n",
    "    max_size = size       # Max size of cluster\n",
    "\n",
    "    hc = Hierarchical_Clustering(ipt_data, max_size)\n",
    "    hc.initialize()\n",
    "    current_clusters = hc.hierarchical_clustering()\n",
    "    return current_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zRL_iShTRub"
   },
   "source": [
    "# Main Method - Run this for output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jJJ-XsTWjDlX",
    "outputId": "40cdf643-ddea-4246-ebf5-1fae53ea9fbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on ind_cent\n",
      "Working on ind_gvea\n"
     ]
    }
   ],
   "source": [
    "size_list=[80,80,1513,1513,8700,8700]\n",
    "for ind in range(len(df_list)):\n",
    "  df=df_list[ind]\n",
    "  size=size_list[ind]\n",
    "  X = df.values[:,:]\n",
    "  print('Working on '+df_string[ind])\n",
    "  current_clusters=MainMethod(X,size)\n",
    "  outputText = \"\"\n",
    "  for centr, valDict in current_clusters.items():\n",
    "    outputText += 'Size:' + str(len(valDict['elements'])) + ' Centroid:' + str(valDict['centroid'][0]) + ' ' + str(valDict['centroid'][1]) + '\\n'\n",
    "    #print('Size:', len(valDict['elements']),'Centroid:', valDict['centroid'])\n",
    "  with open('/content/drive/MyDrive/ACEP_Data_Team/Railbelt_line/Script_data/CSV_files/txt_output/'+df_string[ind]+'_output.txt', 'w') as f:\n",
    "    f.write(outputText)\n",
    "print('!!!!Job finished!!!! Check output folder for TXT files')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "7_TzyqCNL-Ir",
    "7_Gq1j9uTEjE",
    "AteZ6mSfGsSq",
    "yTlX5qvXY9nr",
    "iK5eexkRohe5",
    "aq_BgbphnJT0",
    "0Ec-wwYtQrJv"
   ],
   "name": "Clustering",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
